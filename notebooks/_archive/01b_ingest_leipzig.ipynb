{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b773be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT : d:\\OneDrive\\Documents\\My Learning Resource\\University Courses\\DLSU\\2025-26\\T1\\CSC715M\\assignments\\mc01\n",
      "LEIPZ: d:\\OneDrive\\Documents\\My Learning Resource\\University Courses\\DLSU\\2025-26\\T1\\CSC715M\\assignments\\mc01\\data_raw\\leipzig\n",
      "OUT  : d:\\OneDrive\\Documents\\My Learning Resource\\University Courses\\DLSU\\2025-26\\T1\\CSC715M\\assignments\\mc01\\outputs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, re, csv, json, tarfile, zipfile, gzip, io, html, unicodedata\n",
    "from datetime import datetime\n",
    "\n",
    "def find_root(markers=(\"config.json\",\"corpus.json\")):\n",
    "    p = Path.cwd()\n",
    "    for c in (p, *p.parents):\n",
    "        if any((c/m).exists() for m in markers):\n",
    "            return c\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT   = find_root()\n",
    "RAW    = ROOT/\"data_raw\"\n",
    "LEIPZ  = RAW/\"leipzig\"\n",
    "OUT    = ROOT/\"outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET = 50_000  # word target per rubric\n",
    "\n",
    "print(\"ROOT :\", ROOT)\n",
    "print(\"LEIPZ:\", LEIPZ)\n",
    "print(\"OUT  :\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880fab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_MAP = {\n",
    "    \"tgl\"  : \"tagalog\",\n",
    "    \"ceb\" : \"cebuano\",\n",
    "    \"ilo\" : \"ilocano\",\n",
    "    \"hil\" : \"hiligaynon\",\n",
    "    \"war\" : \"waray\",\n",
    "    \"pam\" : \"kapampangan\",\n",
    "    \"bcl\" : \"bikol\",\n",
    "    \"pag\" : \"pangasinan\",\n",
    "    \"cbk\" : \"chavacano\",\n",
    "    \"eng\" : \"english\",\n",
    "    \"spa\" : \"spanish\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e346cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s:str)->str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\sñáéíóúü-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def guess_key_from_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempt to guess a short language key from a Leipzig archive name.\n",
    "    We pick the first token before '_' if it looks like a code (letters only, 2–5 chars).\n",
    "    e.g., 'tl_news_2012_1M.tar.gz' -> 'tl'\n",
    "          'ceb_wikipedia_2020_300K.zip' -> 'ceb'\n",
    "    \"\"\"\n",
    "    stem = Path(name).name\n",
    "    stem = re.sub(r\"\\.(tar\\.gz|tgz|zip|gz)$\", \"\", stem, flags=re.I)\n",
    "    token = stem.split(\"_\")[0]\n",
    "    token = re.sub(r\"[^A-Za-z]\", \"\", token)\n",
    "    if 2 <= len(token) <= 5:\n",
    "        return token.lower()\n",
    "    return stem.lower()\n",
    "\n",
    "def to_lang_out(archive_name: str) -> str:\n",
    "    key = guess_key_from_name(archive_name)\n",
    "    return MANUAL_MAP.get(key, key)  # fallback to the guessed key if not in MANUAL_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee158a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found archives: 8\n",
      " - bcl_community_2017.tar.gz\n",
      " - ceb_community_2017.tar.gz\n",
      " - eng_wikipedia_2016_10K.tar.gz\n",
      " - ilo_community_2017.tar.gz\n",
      " - pag_community_2017.tar.gz\n",
      " - pam_community_2017.tar.gz\n",
      " - spa_wikipedia_2021_10K.tar.gz\n",
      " - tgl_community_2017.tar.gz\n"
     ]
    }
   ],
   "source": [
    "archives = []\n",
    "if LEIPZ.exists():\n",
    "    for p in sorted(LEIPZ.iterdir()):\n",
    "        if p.suffix.lower() in (\".zip\", \".gz\"):\n",
    "            archives.append(p)\n",
    "        elif p.suffix.lower() == \".tgz\" or str(p).lower().endswith(\".tar.gz\"):\n",
    "            archives.append(p)\n",
    "\n",
    "print(\"Found archives:\", len(archives))\n",
    "for a in archives: print(\" -\", a.name)\n",
    "assert archives, \"Place Leipzig archives into data_raw/leipzig/ and rerun.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9736df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_member(fobj) -> str:\n",
    "    \"\"\"\n",
    "    Read a Leipzig '*-sentences.txt' member file and return plain text (one sentence per line).\n",
    "    Expected format is usually TSV: <id>\\t<sentence>\n",
    "    We keep only the sentence column (index 1) if it exists; otherwise whole line.\n",
    "    \"\"\"\n",
    "    out_lines = []\n",
    "    for raw in fobj:\n",
    "        if isinstance(raw, bytes):\n",
    "            line = raw.decode(\"utf-8\", \"ignore\").rstrip(\"\\n\")\n",
    "        else:\n",
    "            line = raw.rstrip(\"\\n\")\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            out_lines.append(parts[1].strip())\n",
    "        else:\n",
    "            out_lines.append(line.strip())\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "def extract_text_from_archive(arc_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Return concatenated sentences text from a Leipzig archive.\n",
    "    Searches for the first member matching '*-sentences.txt' (or '.txt.gz' inside zip/tar).\n",
    "    \"\"\"\n",
    "    name = arc_path.name.lower()\n",
    "    if name.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(arc_path, \"r\") as zf:\n",
    "            # try uncompressed sentences first\n",
    "            candidates = [m for m in zf.namelist() if m.endswith(\"-sentences.txt\")]\n",
    "            gz_candidates = [m for m in zf.namelist() if m.endswith(\"-sentences.txt.gz\")]\n",
    "            if candidates:\n",
    "                with zf.open(candidates[0], \"r\") as f:\n",
    "                    return read_sentences_from_member(f)\n",
    "            if gz_candidates:\n",
    "                with zf.open(gz_candidates[0], \"r\") as f:\n",
    "                    with gzip.open(io.BytesIO(f.read()), \"rt\", encoding=\"utf-8\", errors=\"ignore\") as gf:\n",
    "                        return read_sentences_from_member(gf)\n",
    "            raise FileNotFoundError(\"No *-sentences.txt found inside ZIP.\")\n",
    "    elif name.endswith(\".tar.gz\") or name.endswith(\".tgz\"):\n",
    "        mode = \"r:gz\"\n",
    "        with tarfile.open(arc_path, mode) as tf:\n",
    "            members = tf.getmembers()\n",
    "            cand = next((m for m in members if m.name.endswith(\"-sentences.txt\")), None)\n",
    "            if cand:\n",
    "                f = tf.extractfile(cand); assert f is not None\n",
    "                return read_sentences_from_member(f)\n",
    "            cand_gz = next((m for m in members if m.name.endswith(\"-sentences.txt.gz\")), None)\n",
    "            if cand_gz:\n",
    "                f = tf.extractfile(cand_gz); assert f is not None\n",
    "                with gzip.open(io.BytesIO(f.read()), \"rt\", encoding=\"utf-8\", errors=\"ignore\") as gf:\n",
    "                    return read_sentences_from_member(gf)\n",
    "            raise FileNotFoundError(\"No *-sentences.txt found inside TAR.GZ.\")\n",
    "    elif name.endswith(\".gz\"):\n",
    "        # Could be a single *-sentences.txt.gz file\n",
    "        with gzip.open(arc_path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as gf:\n",
    "            return read_sentences_from_member(gf)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported archive format: {arc_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf6139dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[bikol] Extracting from bcl_community_2017.tar.gz …\n",
      "  wrote → bikol.txt\n",
      "  words: raw=281,402 | normalized=273,651\n",
      "\n",
      "[cebuano] Extracting from ceb_community_2017.tar.gz …\n",
      "  wrote → cebuano.txt\n",
      "  words: raw=10,666,146 | normalized=10,032,403\n",
      "\n",
      "[english] Extracting from eng_wikipedia_2016_10K.tar.gz …\n",
      "  wrote → english.txt\n",
      "  words: raw=209,518 | normalized=206,131\n",
      "\n",
      "[ilocano] Extracting from ilo_community_2017.tar.gz …\n",
      "  wrote → ilocano.txt\n",
      "  words: raw=391,789 | normalized=381,602\n",
      "\n",
      "[pangasinan] Extracting from pag_community_2017.tar.gz …\n",
      "  wrote → pangasinan.txt\n",
      "  words: raw=64,484 | normalized=58,456\n",
      "\n",
      "[kapampangan] Extracting from pam_community_2017.tar.gz …\n",
      "  wrote → kapampangan.txt\n",
      "  words: raw=300,047 | normalized=288,806\n",
      "\n",
      "[spanish] Extracting from spa_wikipedia_2021_10K.tar.gz …\n",
      "  wrote → spanish.txt\n",
      "  words: raw=208,068 | normalized=203,412\n",
      "\n",
      "[tagalog] Extracting from tgl_community_2017.tar.gz …\n",
      "  wrote → tagalog.txt\n",
      "  words: raw=18,629,220 | normalized=18,651,175\n",
      "\n",
      "Done ingesting Leipzig archives.\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for arc in archives:\n",
    "    lang_out = to_lang_out(arc.name)\n",
    "    raw_txt_path = RAW / f\"{lang_out}.txt\"\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n[{lang_out}] Extracting from {arc.name} …\")\n",
    "        text = extract_text_from_archive(arc)\n",
    "        if not text.strip():\n",
    "            print(f\"  [warn] Empty text extracted from {arc.name}. Skipping write.\")\n",
    "            raw_count = 0\n",
    "            norm_count = 0\n",
    "        else:\n",
    "            # Write raw extracted sentences (one per line)\n",
    "            raw_txt_path.write_text(text, encoding=\"utf-8\")\n",
    "            # Counts\n",
    "            raw_count  = len(text.split())\n",
    "            norm_count = len(normalize(text).split())\n",
    "            print(f\"  wrote → {raw_txt_path.name}\")\n",
    "            print(f\"  words: raw={raw_count:,} | normalized={norm_count:,}\")\n",
    "\n",
    "        meets = norm_count >= TARGET\n",
    "        summary.append({\n",
    "            \"archive\": arc.name,\n",
    "            \"lang_out\": lang_out,\n",
    "            \"raw_words\": raw_count,\n",
    "            \"normalized_words\": norm_count,\n",
    "            \"meets_50k\": bool(meets),\n",
    "            \"output_path\": str(raw_txt_path),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  [error] {arc.name}: {e}\")\n",
    "        summary.append({\n",
    "            \"archive\": arc.name,\n",
    "            \"lang_out\": lang_out,\n",
    "            \"raw_words\": 0,\n",
    "            \"normalized_words\": 0,\n",
    "            \"meets_50k\": False,\n",
    "            \"output_path\": str(raw_txt_path),\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "print(\"\\nDone ingesting Leipzig archives.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
